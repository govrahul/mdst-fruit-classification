{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install Dependencies and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 Let's setup Kaggle**\n",
    "\n",
    "1. Visit https://www.kaggle.com/\n",
    "2. Create an account\n",
    "3. Go to 'Settings'\n",
    "4. Under the 'Account' tab, select 'Create New Token'\n",
    "5. Kaggle will download a kaggle.json object.\n",
    "6. Relocate this object from 'Downloads' to ~/.kaggle (Kaggle will look for the object at ~/.kaggle/kaggle.json)\n",
    "\n",
    "Now, your terminal will have access to the 'kaggle' keyword\n",
    "\n",
    "**1.2 Now, setup the python environment**\n",
    "\n",
    "1. Download python3 if you don't have it installed\n",
    "2. Run: `python3 -m venv env`\n",
    "3. Run: `source env/bin/activate`\n",
    "\n",
    "You should now be inside running a python virtual environment\n",
    "\n",
    "**1.3 once in the virtual environment, we will install dependencies and fetch our database!**\n",
    "\n",
    "1. Run: `pip install -r requirements.txt`\n",
    "2. Run: `kaggle datasets download -d moltean/fruits`\n",
    "3. Run: `unzip fruits.zip`\n",
    "\n",
    "**•You should now have both the resized 100x100 dataset and the original size dataset as directories in your working directory.**\n",
    "\n",
    "**Note: it makes sense to just use the 100x100 dataset to reduce training time.**\n",
    "\n",
    "**It also has way more classes than the full-size set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have all the requirements installed, we should be able to do the following:\n",
    "\n",
    "Note: The first time you do this in Visual Studio Code, it may ask you which Python environment to use. Select the one you previously initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "# This was in a tutorial. Not sure if it's really necessary but run just in case..\n",
    "\n",
    "## IMPORTANT: If you are on Windows, uncomment this line: ##\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shows us which GPUs our system has access to. It's okay if you don't have any.\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Check out our training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Change 'data' to the name of your training set directory\n",
    "# You should see a list of classes\n",
    "data_dir = 'data'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This formats our data...\n",
    "# TODO: Ensure the image size is kept at (100, 100)\n",
    "data = tf.keras.utils.image_dataset_from_directory(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each time we call this, it gives us a new set of data\n",
    "data_iterator = data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 32 images per batch, 100x100, 3 channels (R, G, B)\n",
    "batch = data_iterator.next()\n",
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Scale Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Our tensorflow model works with values between 0 and 1.\n",
    "2. Our images give us pixel R, G, B values from 0-255.\n",
    "\n",
    "Thus, we need to scale our input data down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Pixel values range from 0-255. We want to scale x to range between 0-1.\n",
    "# x represents our data, and y represents our class. Therefore, we shouldn't worry about y\n",
    "\n",
    "# TODO: Uncomment + complete the following statement:\n",
    "# data = data.map(lambda x,y: (x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This will now give us an iterator with our SCALED data!\n",
    "scaled_iterator = data.as_numpy_iterator()\n",
    "batch = scaled_iterator.next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once previous TODOs are complete, you should see 4 100x100 images here (of fruits, hopefully)\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx, img in enumerate(batch[0][:4]):\n",
    "    ax[idx].imshow(img)\n",
    "    ax[idx].title.set_text(batch[1][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Include Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Now, it's your turn. Do the same steps, except this time with the testing directory...\n",
    "**Note: for functionality, you'll only need to pattern-match some of the lines**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Create Validation Set\n",
    "•Now, we're gonna do something funky!\n",
    "\n",
    "•Usually, we would have a training, a validation, and a testing set.\n",
    "\n",
    "•With the 100x100 fruits dataset, we are missing a validation set...\n",
    "\n",
    "To fix this, we are going to create our validation set by stealing some images from our training set!\n",
    "\n",
    "However... this can result in issues!\n",
    "It may result in training data leaving out some of our classes (fruits)...\n",
    "\n",
    "As an exercise, you can think about what you can do to solve this..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data) * 0.7)\n",
    "val_size = int(len(data) * 0.3)\n",
    "# Leave our test data alone\n",
    "\n",
    "# TODO: put in the name of your test_data here\n",
    "test_size = int(len(test_data))\n",
    "\n",
    "# TODO: Make sure train_size + val_size + test_size lines up with the total size of your data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice how we separate the training + validation data...\n",
    "\n",
    "train = data.take(train_size)\n",
    "val = data.skip(train_size).take(val_size)\n",
    "test = test_data.take(test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Build Deep Learning Model\n",
    "(We will get into this more the second week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add in all your layers here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir='logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Complete the arguments for model.fit()\n",
    "hist = model.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
